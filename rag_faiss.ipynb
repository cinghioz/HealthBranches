{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.document_loaders import JSONLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from tqdm.notebook import tqdm\n",
    "import faiss\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "import glob\n",
    "\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "from prompt import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def count_chars_in_files(folder_path):\n",
    "#     file_lengths = []\n",
    "#     small_files = []\n",
    "\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         if filename.endswith(\".txt\"):  # Process only .txt files\n",
    "#             file_path = os.path.join(folder_path, filename)\n",
    "#             with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#                 char_count = len(file.read())\n",
    "#                 file_lengths.append(char_count)\n",
    "#                 if char_count > 10000:\n",
    "#                     small_files.append((filename, char_count))\n",
    "\n",
    "#     if file_lengths:\n",
    "#         min_chars = min(file_lengths)\n",
    "#         max_chars = max(file_lengths)\n",
    "#         mean_chars = sum(file_lengths) / len(file_lengths)\n",
    "\n",
    "#         print(f\"Min: {min_chars} chars\")\n",
    "#         print(f\"Max: {max_chars} chars\")\n",
    "#         print(f\"Mean: {mean_chars:.2f} chars\")\n",
    "\n",
    "#         if small_files:\n",
    "#             print(\"\\nFiles with fewer than 10000 characters:\")\n",
    "#             for filename, char_count in small_files:\n",
    "#                 print(f\"{filename}: {char_count} chars\")\n",
    "#         else:\n",
    "#             print(\"\\nNo files with fewer than 100 characters.\")\n",
    "#     else:\n",
    "#         print(\"No .txt files found in the folder.\")\n",
    "\n",
    "# # Example usage\n",
    "# folder_path = \"/home/cc/PHD/ragkg/data/kgbase-new\"\n",
    "# count_chars_in_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the input CSV\n",
    "# df = pd.read_csv('/home/cc/PHD/ragkg/closed_questions/merged_output.csv')\n",
    "\n",
    "# # Define a function to transform the 'qea' string\n",
    "# def transform_qea(qea_str):\n",
    "#     question_match = re.search(r'Question:\\s*(.*?)\\nA:', qea_str, re.S)\n",
    "#     question = question_match.group(1).strip() if question_match else \"\"\n",
    "\n",
    "#     # Extract choices and convert to a list (removing letters A:, B:, etc.)\n",
    "#     choices_match = re.findall(r'[A-D]:\\s*(.*?)(?=\\n[A-D]:|\\nCorrect answer:|\\Z)', qea_str, re.S)\n",
    "#     choices = [choice.strip() for choice in choices_match]\n",
    "\n",
    "#     # Extract correct answer\n",
    "#     correct_match = re.search(r'Correct answer:\\s*([A-D])', qea_str)\n",
    "#     correct_option = correct_match.group(1).strip() if correct_match else \"\"\n",
    "    \n",
    "#     return question, choices, correct_option\n",
    "\n",
    "# # Create new columns based on the 'qea' column\n",
    "# df[['question', 'choices', 'correct_option']] = df['qea'].apply(lambda x: pd.Series(transform_qea(x)))\n",
    "\n",
    "# # Now, you can save the new CSV with the required columns\n",
    "# df = df[['question', 'choices', 'correct_option', 'paths', 'name']]\n",
    "\n",
    "# # Save the result to a new CSV\n",
    "# df.to_csv('/home/cc/PHD/ragkg/closed_questions/old_quiz.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, index_path: str, embedder_name: str = \"mxbai-embed-large\"):\n",
    "        self.index_path = index_path\n",
    "        self.embedder_name = embedder_name\n",
    "        self.embedder = OllamaEmbeddings(model=embedder_name)\n",
    "        self._load_vector_store()\n",
    "\n",
    "    def _load_vector_store(self):\n",
    "        if os.path.exists(self.index_path):\n",
    "            print(\"### LOAD VECTOR DB ###\")\n",
    "\n",
    "            self.index = faiss.read_index(self.index_path+'index.faiss')\n",
    "            \n",
    "            with open(self.index_path+'doc_to_id.pkl', \"rb\") as f:\n",
    "                self.index_to_doc_id = pickle.load(f)\n",
    "\n",
    "            with open(self.index_path+'docstore.pkl', \"rb\") as f:\n",
    "                self.docstore = pickle.load(f)\n",
    "\n",
    "            self.vector_store = FAISS(\n",
    "                embedding_function=self.embedder,\n",
    "                index=self.index,\n",
    "                docstore=self.docstore,\n",
    "                index_to_docstore_id=self.index_to_doc_id\n",
    "            )   \n",
    "        else:\n",
    "            print(\"### CREATE VECTOR DB ###\")\n",
    "\n",
    "            self.index = faiss.IndexFlatL2(len(self.embedder.embed_query('hello world')))\n",
    "            self.index_to_doc_id = {}\n",
    "            self.docstore = InMemoryDocstore()\n",
    "\n",
    "            self.vector_store = FAISS(\n",
    "                embedding_function=self.embedder,\n",
    "                index=self.index,\n",
    "                docstore=self.docstore,\n",
    "                index_to_docstore_id=self.index_to_doc_id\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(self.index_path):\n",
    "                os.makedirs(self.index_path)\n",
    "\n",
    "    def _load_documents(self, doc_path: str, doc_type: str = \"*.txt\") -> list[Document]:\n",
    "        loader = DirectoryLoader(doc_path, glob=doc_type)\n",
    "        documents = loader.load()\n",
    "        return documents\n",
    "\n",
    "    def _split_text(self, documents: list[Document]):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=150,\n",
    "            length_function=len,\n",
    "            add_start_index=True,\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def add_documents(self, doc_path: str, doc_type: str = \"*.txt\"):\n",
    "        documents = self._load_documents(doc_path, doc_type)\n",
    "        chunks = self._split_text(documents)\n",
    "        self.vector_store.add_documents(documents=chunks)\n",
    "        self._update_vector_db()\n",
    "    \n",
    "    def search(self, query: str, k: int = 3):\n",
    "        return self.vector_store.similarity_search(query=query, k=k)\n",
    "        # return self.vector_store.similarity_search(query=self._transform_query(query), k=3)\n",
    "\n",
    "    def _update_class(self):\n",
    "        self.index = self.vector_store.index\n",
    "        self.index_to_doc_id = self.vector_store.index_to_docstore_id\n",
    "        self.docstore = self.vector_store.docstore\n",
    "    \n",
    "    def _update_vector_db(self):\n",
    "        faiss.write_index(self.vector_store.index, self.index_path+'index.faiss')\n",
    "\n",
    "        with open(self.index_path+'doc_to_id.pkl', \"wb\") as f:\n",
    "            pickle.dump(self.vector_store.index_to_docstore_id, f)\n",
    "\n",
    "        with open(self.index_path+'docstore.pkl', \"wb\") as f:\n",
    "            pickle.dump(self.vector_store.docstore, f)\n",
    "\n",
    "        self._update_class()\n",
    "\n",
    "        print(\"### UPDATE VECTOR DB ###\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty vector store in the indicated path. If the path already exists, load the vector store\n",
    "vector_store = VectorStore('/home/cc/PHD/ragkg/indexes/kgbase/')\n",
    "\n",
    "# Add documents in vector store (comment this line after the first add)\n",
    "# vector_store.add_documents('/home/cc/PHD/ragkg/data/kgbase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMinference:\n",
    "    def __init__(self, llm_name):\n",
    "        self.llm_name = llm_name\n",
    "        self.model = OllamaLLM(model=llm_name) \n",
    "\n",
    "    def _transform_query(self, query: str) -> str:\n",
    "        return f'Represent this sentence for searching relevant passages: {query}'\n",
    "\n",
    "    def single_inference(self, query: str, template: str, path: str,  choices: List[str], cond: str,  context) -> str | List[str]:\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in context])\n",
    "\n",
    "        prompt_template = ChatPromptTemplate.from_template(template)\n",
    "        if path != \"\":\n",
    "            prompt = prompt_template.format(context=context_text, question=query, path=path, condition=cond, \n",
    "                                            o1=choices[0], o2=choices[1], o3=choices[2], o4=choices[3], o5=choices[4])\n",
    "        else:\n",
    "            prompt = prompt_template.format(context=context_text, question=query, condition=cond, o1=choices[0], \n",
    "                                            o2=choices[1], o3=choices[2], o4=choices[3], o5=choices[4])\n",
    "\n",
    "        response_text = self.model.invoke(prompt)\n",
    "        response_text = response_text.strip().replace(\"\\n\", \"\").replace(\"  \", \"\")\n",
    "\n",
    "        sources = [doc.metadata.get(\"source\", None) for doc in context]\n",
    "        \n",
    "        return response_text, sources\n",
    "\n",
    "    def qea_evaluation(self, query: str, template: str, path: str, choices: List[str], cond: str,  vector_store):\n",
    "\n",
    "        results = vector_store.search(query=query, k=5)\n",
    "\n",
    "        response, sources = self.single_inference(query, template, path, choices, cond, results)\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/home/cc/PHD/ragkg/MedQA\"\n",
    "# models = [\"mistral\", \"llama3.1:8b\", \"llama2:7b\", \"medllama2:7b\", \"gemma:7b\", \"gemma2:9b\", \"phi4:14b\", \"qwen2.5:7b\", \"mixtral:8x7b\", \"deepseek-r1:7b\"]\n",
    "models = [\"mistral\"]\n",
    "\n",
    "# templates = [PROMPT_TEMPLATE, PROMPT_TEMPLATE_ONE, PROMPT_TEMPLATE_RAG]\n",
    "templates = [PROMPT_MED, PROMPT_MED_RAG]\n",
    "\n",
    "med_files = glob.glob(f\"{folder_path}/*/top*\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_rag = 0\n",
    "cnt = 0\n",
    "\n",
    "rows = []\n",
    "\n",
    "for model in models:\n",
    "    model_name = model\n",
    "    llm = LLMinference(llm_name=model_name)\n",
    "\n",
    "    cnt = 0\n",
    "    rows = []\n",
    "\n",
    "    for jso in tqdm(med_files):\n",
    "        questions = pd.read_json(jso, lines=True)\n",
    "\n",
    "        for index, row in questions.iterrows():\n",
    "            res = []\n",
    "\n",
    "            cond = jso.split('/')[-2].lower()\n",
    "\n",
    "            for template in templates:\n",
    "                res.append(llm.qea_evaluation(row['question'], template, \"\", list(row['options'].values()), cond, vector_store))\n",
    "                # res.append(llm.qea_evaluation(row['question'], template, row['reasoning_trace'], ast.literal_eval(row['choices']), row['name'].lower(), vector_store)) # Baseline\n",
    "\n",
    "            res.append(row[\"answer_idx\"])\n",
    "            res.append(row['question'])\n",
    "            res.insert(0, cond)\n",
    "\n",
    "            rows.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows, columns=[\"name\", \"zero_shot\", \"zero_shot_rag\", \"real\", \"question\"]) # medqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"/home/cc/PHD/ragkg/results_medqa_{model_name}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
