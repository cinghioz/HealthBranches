{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "# from together import Together\n",
    "from alive_progress import alive_bar\n",
    "from collections import Counter\n",
    "\n",
    "from prompt import *\n",
    "from utils import extract_option\n",
    "\n",
    "PATH = \"/home/cc/PHD/HealthBranches/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtc = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_to_check.csv\")\n",
    "qc = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_checked.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_answers(file_path: str, model: str):\n",
    "    df = pd.read_csv(file_path)\n",
    "    accs = []\n",
    "    \n",
    "    for col in [c for c in df.columns if c.startswith((\"zero_shot\", \"one_shot\"))]:\n",
    "        df[f'{col}_choice'] = df[col].apply(extract_option)\n",
    "        df[f'{col}_is_correct'] = df[f'{col}_choice'] == df['real']\n",
    "        accuracy = df[f'{col}_is_correct'].mean()\n",
    "        accs.append(accuracy)\n",
    "    \n",
    "    accs.insert(0, model)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_path = \"/home/cc/PHD/HealthBranches/results_misc/results_QUIZ_bench_\"\n",
    "base_path = \"/home/cc/PHD/HealthBranches/results_misc/results_QUIZ_baseline2_\"\n",
    "models = ['gemma2_9b']\n",
    "\n",
    "bench = [_evaluate_answers(f\"{bench_path}{model}.csv\", model) for model in models]\n",
    "baseline = [_evaluate_answers(f\"{base_path}{model}.csv\", model) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[['gemma2_9b', np.float64(0.685), np.float64(0.707)]]\n",
    "[['gemma2_9b', np.float64(0.928), np.float64(0.699), np.float64(0.875)]] #new path\n",
    "[['gemma2_9b', np.float64(0.914), np.float64(0.695), np.float64(0.877)]] #old path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT PRIMA E DOPO DOMANDE DIFF ###\n",
    "\n",
    "bench_old = [['mistral:7b',\n",
    "  np.float64(0.1970074812967581),\n",
    "  np.float64(0.23524522028262676)],\n",
    " ['gemma:7b', np.float64(0.24522028262676643), np.float64(0.2743142144638404)],\n",
    " ['gemma2:9b',\n",
    "  np.float64(0.15793848711554448),\n",
    "  np.float64(0.26101413133832085)],\n",
    " ['gemma3:4b',\n",
    "  np.float64(0.2194513715710723),\n",
    "  np.float64(0.25353283458021614)],\n",
    " ['llama3.1:8b',\n",
    "  np.float64(0.1770573566084788),\n",
    "  np.float64(0.21529509559434745)],\n",
    " ['qwen2.5:7b',\n",
    "  np.float64(0.1886949293433084),\n",
    "  np.float64(0.2502078137988362)],\n",
    " ['llama2:7b',\n",
    "  np.float64(0.18952618453865336),\n",
    "  np.float64(0.20199501246882792)]]\n",
    "\n",
    "bench_new = [['mistral:7b',\n",
    "  np.float64(0.3475238922675934),\n",
    "  np.float64(0.42224152910512597)],\n",
    " ['gemma:7b',\n",
    "  np.float64(0.38835794960903564),\n",
    "  np.float64(0.41876629018245004)],\n",
    " ['gemma2:9b', np.float64(0.4291920069504778), np.float64(0.4630755864465682)],\n",
    " ['gemma3:4b', np.float64(0.4144222415291051), np.float64(0.4283231972198089)],\n",
    " ['llama3.1:8b',\n",
    "  np.float64(0.40834057341442226),\n",
    "  np.float64(0.45091225021720244)],\n",
    " ['qwen2.5:7b',\n",
    "  np.float64(0.45351867940920937),\n",
    "  np.float64(0.48479582971329277)],\n",
    " ['llama2:7b',\n",
    "  np.float64(0.17115551694178974),\n",
    "  np.float64(0.26151172893136404)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_neurips_style():\n",
    "    \"\"\"\n",
    "    Apply NeurIPS-friendly styling to matplotlib plots with seaborn darkgrid aesthetics\n",
    "    and Computer Modern fonts\n",
    "    \"\"\"\n",
    "    # Add Computer Modern font family\n",
    "    # This assumes the user has the Computer Modern fonts installed in the system\n",
    "    # If not, you might need to install them or use matplotlib's builtin 'cm' family\n",
    "    try:\n",
    "        # Try to use the proper Computer Modern fonts if available\n",
    "        plt.rcParams['font.family'] = 'serif'\n",
    "        plt.rcParams['font.serif'] = ['Computer Modern Roman'] + plt.rcParams['font.serif']\n",
    "        plt.rcParams['mathtext.fontset'] = 'cm'  # Use Computer Modern math font\n",
    "    except:\n",
    "        # Fallback to matplotlib's built-in Computer Modern\n",
    "        plt.rcParams['font.family'] = 'serif'\n",
    "        plt.rcParams['mathtext.fontset'] = 'cm'\n",
    "    \n",
    "    # Figure size for NeurIPS papers (designed to fit in a single column)\n",
    "    plt.rcParams['figure.figsize'] = (3.5, 2.625)  # 3.5 x 2.625 inches is good for a single column\n",
    "    \n",
    "    # Font sizes\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    plt.rcParams['axes.titlesize'] = 12\n",
    "    plt.rcParams['axes.labelsize'] = 12\n",
    "    plt.rcParams['xtick.labelsize'] = 12\n",
    "    plt.rcParams['ytick.labelsize'] = 12\n",
    "    plt.rcParams['legend.fontsize'] = 12\n",
    "    \n",
    "    # Line widths\n",
    "    plt.rcParams['axes.linewidth'] = 0.8\n",
    "    plt.rcParams['lines.linewidth'] = 1.5\n",
    "    plt.rcParams['grid.linewidth'] = 0.5\n",
    "    \n",
    "    # Marker size and style\n",
    "    plt.rcParams['lines.markersize'] = 4\n",
    "    plt.rcParams['scatter.marker'] = 'o'\n",
    "    \n",
    "    # Seaborn-inspired colors - maintains colorblind friendliness\n",
    "    plt.rcParams['axes.prop_cycle'] = plt.cycler('color', [\n",
    "        '#4C72B0',  # blue\n",
    "        '#DD8452',  # orange\n",
    "        '#55A868',  # green\n",
    "        '#C44E52',  # red\n",
    "        '#8172B3',  # purple\n",
    "        '#937860',  # brown\n",
    "        '#DA8BC3',  # pink\n",
    "        '#8C8C8C',  # gray\n",
    "        '#CCB974',  # khaki\n",
    "        '#64B5CD',  # light blue\n",
    "    ])\n",
    "    \n",
    "    # Darkgrid background and grid (seaborn style)\n",
    "    plt.rcParams['axes.facecolor'] = '#EAEAF2'  # light gray background\n",
    "    plt.rcParams['figure.facecolor'] = 'white'\n",
    "    plt.rcParams['grid.color'] = 'white'\n",
    "    plt.rcParams['grid.alpha'] = 1.0\n",
    "    plt.rcParams['grid.linewidth'] = 1.0\n",
    "    \n",
    "    # Spines and ticks - seaborn darkgrid typically has reduced spines\n",
    "    plt.rcParams['axes.spines.top'] = False\n",
    "    plt.rcParams['axes.spines.right'] = False\n",
    "    plt.rcParams['axes.spines.left'] = True\n",
    "    plt.rcParams['axes.spines.bottom'] = True\n",
    "    # plt.rcParams['axes.spines.color'] = ['#CCCCCC']  # Light gray spines\n",
    "    \n",
    "    plt.rcParams['xtick.direction'] = 'out'\n",
    "    plt.rcParams['ytick.direction'] = 'out'\n",
    "    plt.rcParams['xtick.major.width'] = 0.8\n",
    "    plt.rcParams['ytick.major.width'] = 0.8\n",
    "    plt.rcParams['xtick.color'] = '#555555'\n",
    "    plt.rcParams['ytick.color'] = '#555555'\n",
    "    \n",
    "    # Legend\n",
    "    plt.rcParams['legend.frameon'] = True\n",
    "    plt.rcParams['legend.framealpha'] = 0.9\n",
    "    plt.rcParams['legend.edgecolor'] = '#CCCCCC'\n",
    "    \n",
    "    # LaTeX-like rendering for text with Computer Modern font\n",
    "    plt.rcParams['text.usetex'] = True  # Set to True if you have LaTeX installed\n",
    "    \n",
    "    # Saving options\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['savefig.bbox'] = 'tight'\n",
    "    plt.rcParams['savefig.pad_inches'] = 0.05\n",
    "\n",
    "    # set figure dpi\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "def use_latex_with_computer_modern():\n",
    "    \"\"\"\n",
    "    Alternative function to use actual LaTeX for text rendering.\n",
    "    This requires a working LaTeX installation with the Computer Modern fonts.\n",
    "    \"\"\"\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Computer Modern Roman\"],\n",
    "    })\n",
    "    \n",
    "    # Call the rest of the styles\n",
    "    set_neurips_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_models(bench_old: list[list], bench_new: list[list]):\n",
    "    use_latex_with_computer_modern()\n",
    "\n",
    "    # Sort both datasets by the first metric (e.g., Zero Shot) ascending\n",
    "    bench_old_sorted = sorted(bench_old, key=lambda x: x[1])\n",
    "    bench_new_sorted = sorted(bench_new, key=lambda x: x[1])\n",
    "\n",
    "    # Extract labels and values for old data\n",
    "    labels = [entry[0] for entry in bench_old_sorted]\n",
    "    old_vs1 = [entry[1]*100 for entry in bench_old_sorted]\n",
    "    old_vs2 = [entry[2]*100 for entry in bench_old_sorted]\n",
    "\n",
    "    # Map new data to the old labels order\n",
    "    new_map = {entry[0]: entry for entry in bench_new_sorted}\n",
    "    new_vs1 = [new_map[name][1]*100 for name in labels]\n",
    "    new_vs2 = [new_map[name][2]*100 for name in labels]\n",
    "\n",
    "    # Bar settings\n",
    "    width = 0.15  # narrower bars to fit five series\n",
    "    num_series = 4\n",
    "\n",
    "    # Spacing between groups\n",
    "    group_spacing = width * (num_series+0.75)\n",
    "    x = np.arange(len(labels)) * group_spacing\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # Plot grouped bars: old/new × two metrics\n",
    "    bars_old_vs1 = ax.bar(x - 1.5*width, old_vs1, width, label=\"Old Zero Shot\")\n",
    "    bars_new_vs1 = ax.bar(x - 0.5*width, new_vs1, width, label=\"New Zero Shot\")\n",
    "    bars_old_vs2 = ax.bar(x + 0.5*width, old_vs2, width, label=\"Old Zero Shot + RAG\")\n",
    "    bars_new_vs2 = ax.bar(x + 1.5*width, new_vs2, width, label=\"New Zero Shot + RAG\")\n",
    "\n",
    "    # Configure ticks and labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=\"vertical\", ha=\"center\")\n",
    "    ax.set_xlabel(\"Models\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "    # Place legend inside plot at bottom right\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "    # Annotate bars with values (no leading zero)\n",
    "    for bars, vals in zip(\n",
    "        (bars_old_vs1, bars_new_vs1, bars_old_vs2, bars_new_vs2),\n",
    "        (old_vs1, new_vs1, old_vs2, new_vs2)\n",
    "    ):\n",
    "        labels_txt = [f\"{v:.0f}\" for v in vals]\n",
    "        ax.bar_label(bars, labels=labels_txt, padding=2, fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plot_models_diff.png\", dpi=500, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_models(bench_old, bench_new)\n",
    "\n",
    "### PLOT PRIMA E DOPO DOMANDE DIFF (FINE) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_new = pd.read_csv(\"/home/cc/PHD/HealthBranches/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = pd.read_csv(\"/home/cc/PHD/HealthBranches/results_misc/results_QUIZ_baseline_gemma2_9b.csv\")\n",
    "path2 = pd.read_csv(\"/home/cc/PHD/HealthBranches/results_misc/results_QUIZ_bench_gemma2_9b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = np.random.randint(0, 1000)\n",
    "\n",
    "print(path1.loc[sam]['path'])\n",
    "print(path2.loc[sam]['path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"/home/cc/PHD/HealthBranches/results/results_QUIZ_bench_qwen2.5_72b.csv\")\n",
    "cnt = 0\n",
    "\n",
    "for index, row in ds.iterrows():\n",
    "    if extract_option(row['zero_shot']) not in ['A', 'B', 'C', 'D', 'E']:\n",
    "        cnt +=1\n",
    "        print(row['zero_shot'])\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/cc/PHD/HealthBranches/results/results_QUIZ_bench_qwen2.5_7b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute path length as the number of '->' in each string\n",
    "df['path_len'] = df['path'].str.count(r'->')\n",
    "\n",
    "# 3. Group by that new column\n",
    "grouped = df.groupby('path_len')\n",
    "\n",
    "# 4a. If you just want to see group sizes:\n",
    "print(\"Rows per path-length:\")\n",
    "print(grouped.size().sort_index())\n",
    "\n",
    "# 4b. If you want to iterate through each group:\n",
    "for length, group in grouped:\n",
    "    print(f\"\\n=== Path length = {length} (i.e. {length} '->' separators) ===\")\n",
    "    print(group[['path']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute path length (number of '->')\n",
    "df['path_len'] = df['path'].str.count(r'->')\n",
    "\n",
    "# 3. Apply extract_option to 'answer' and compare to 'real'\n",
    "#    (Assuming extract_option is already defined or imported)\n",
    "df['pred'] = df['zero_shot'].apply(extract_option)\n",
    "df['correct'] = df['pred'] == df['real']\n",
    "\n",
    "# 4. Group by path length and count matches & totals\n",
    "agg = (\n",
    "    df\n",
    "    .groupby('path_len')\n",
    "    .agg(\n",
    "        total_rows=('correct','size'),\n",
    "        correct_count=('correct','sum')\n",
    "    )\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "total_rows_all = len(df)\n",
    "agg['accuracy'] = agg['correct_count'] / total_rows_all\n",
    "\n",
    "# 7) Plot bar chart of weighted accuracies\n",
    "labels = [str(length) for length in agg.index]\n",
    "weighted_accuracies = agg['accuracy']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(labels, weighted_accuracies)\n",
    "plt.xlabel('Path Length (number of \"->\" separators)')\n",
    "plt.ylabel('Weighted Accuracy\\n(corrects ÷ total rows)')\n",
    "plt.title('Weighted Accuracy by Path Length')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Annotate bars with percentages\n",
    "for bar, val in zip(bars, weighted_accuracies):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        val + 0.005,\n",
    "        f\"{val:.0%}\",\n",
    "        ha='center',\n",
    "        va='bottom'\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_pro/final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Carica il CSV con le domande da escludere\n",
    "df_blocklist = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_to_remove.csv\")\n",
    "\n",
    "# 2) Carica il CSV originale che vuoi filtrare\n",
    "df_orig = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_pro/dataset_updated_V2path.csv\")\n",
    "\n",
    "# 3) Filtra: tieni solo le righe il cui 'question' NON è nella blocklist\n",
    "df_filtrato = df_orig[~df_orig['question'].isin(df_blocklist['question'])]\n",
    "\n",
    "# 4) Scrivi il risultato su un nuovo CSV\n",
    "df_filtrato.to_csv('tutte_le_domande_filtrate.csv', index=False)\n",
    "print(len(df_filtrato))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_options(opt_str):\n",
    "    \"\"\"\n",
    "    Ritorna True se opt_str può essere convertito in lista di lunghezza 5,\n",
    "    altrimenti False (anche in caso di eccezione di parsing).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Normalizza le virgolette e tenta l'ast.literal_eval\n",
    "        opts = ast.literal_eval(\n",
    "            opt_str\n",
    "            .replace(\"['\", '[\"')\n",
    "            .replace(\"']\", '\"]')\n",
    "            .replace(\"', '\", '\", \"')\n",
    "        )\n",
    "        # Controlla che sia lista di lunghezza 5\n",
    "        return isinstance(opts, list) and len(opts) == 5\n",
    "    except (ValueError, SyntaxError):\n",
    "        return False\n",
    "\n",
    "# Applichiamo il filtro sul DataFrame originale\n",
    "mask = df['options'].apply(is_valid_options)\n",
    "\n",
    "# df filtrato: mantengo solo le righe valide\n",
    "df = df.loc[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/home/cc/PHD/HealthBranches/questions_pro/dataset_updated_V2path.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for c in df['correct_option'].values:\n",
    "    if c not in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n",
    "        print(i)\n",
    "        print(df.loc[i]['correct_option'])\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{PATH}results/results_quiz_Llama-3.3-70B-Instruct-Turbo-Free.csv\")\n",
    "df[\"zero_shot_new\"] = df[\"zero_shot\"].apply(extract_option)\n",
    "\n",
    "big_filtered = df[df[\"zero_shot_new\"].str.upper() != df[\"real\"].str.upper()]\n",
    "len(big_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = pd.read_csv(f\"{PATH}results/results_quiz_mistral.csv\")\n",
    "small_df = small_df[small_df[\"question\"].isin(big_filtered[\"question\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df[\"zero_shot_new\"] = small_df[\"zero_shot\"].apply(extract_option)\n",
    "small_filtered = small_df[small_df[\"zero_shot_new\"].str.upper() != small_df[\"real\"].str.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory contenente i file CSV\n",
    "csv_directory = \"/home/cc/PHD/HealthBranches/quiz-ultimate-dataset\"\n",
    "\n",
    "# Leggere tutti i CSV\n",
    "csv_files = [os.path.join(csv_directory, f) for f in os.listdir(csv_directory) if f.endswith(\".csv\")]\n",
    "questions = []\n",
    "\n",
    "print(csv_files)\n",
    "\n",
    "# Elaborazione di ogni CSV\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Rimuove la colonna \"zero_shot_rag\" se esiste\n",
    "    if \"zero_shot_rag\" in df.columns:\n",
    "        df = df.drop(columns=[\"zero_shot_rag\"])\n",
    "\n",
    "    # Applica la funzione di estrazione su zero_shot\n",
    "    df[\"zero_shot\"] = df[\"zero_shot\"].apply(extract_option)\n",
    "\n",
    "    # Filtra le righe in cui zero_shot è diverso da real\n",
    "    df_filtered = df[df[\"zero_shot\"] != df[\"real\"]].copy()\n",
    "\n",
    "    questions.extend(df_filtered['question'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conta le occorrenze\n",
    "conteggio = Counter(questions)\n",
    "\n",
    "# Ordina in ordine decrescente\n",
    "dizionario_ordinato = dict(sorted(conteggio.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "filtered_dict = {k: v for k, v in dizionario_ordinato.items() if v >= 6}\n",
    "\n",
    "print(len(filtered_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_to_check = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_to_check.csv\")\n",
    "\n",
    "# Trova le domande che sono sia nel dizionario che nel DataFrame\n",
    "matching_questions = [q for q in filtered_dict.keys() if q in questions_to_check[\"question\"].values]\n",
    "print(len(matching_questions))\n",
    "\n",
    "print(\"Domande trovate nel DataFrame:\", len(matching_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il CSV (sostituisci 'file.csv' con il tuo file)\n",
    "df = pd.read_csv(\"/home/cc/PHD/HealthBranches/results/results_checker_70B.csv\")\n",
    "\n",
    "# Conta le occorrenze di ogni valore nella colonna \"check\"\n",
    "conteggio = df[\"check\"].value_counts().sort_index()\n",
    "\n",
    "# Stampa il risultato\n",
    "print(conteggio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Se la GPU è disponibile, usa 'cuda', altrimenti 'cpu'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Esecuzione su: {device}\")\n",
    "\n",
    "# Carica il modello sulla GPU se disponibile\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=device)\n",
    "\n",
    "# Sostituisci 'data.csv' con il percorso del tuo file CSV.\n",
    "df = pd.read_csv('/home/cc/PHD/HealthBranches/questions_pro/dataset_updated_V2path.csv')\n",
    "\n",
    "# Definisce la soglia di similarità (80%)\n",
    "SIMILARITY_THRESHOLD = 0.9\n",
    "\n",
    "# Pre-elabora il DataFrame:\n",
    "# 1. Dividi il campo 'path' in una lista di segmenti\n",
    "df['path_segments'] = df['path'].apply(lambda x: [seg.strip() for seg in x.split(\"->\")])\n",
    "# 2. Estrai il testo dell'opzione corretta\n",
    "def extract_correct_option(row):\n",
    "    try:\n",
    "        options_list = ast.literal_eval(row['options'].replace(\"['\", '[\"').replace(\"']\", '\"]').replace(\"', '\", '\", \"'))\n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        # Se il parsing fallisce, restituisce una stringa vuota\n",
    "        return \"\"\n",
    "    correct_letter = row['correct_option'].strip().upper()\n",
    "    index = ord(correct_letter) - ord('A')\n",
    "    if index < 0 or index >= len(options_list):\n",
    "        return \"\"\n",
    "    return options_list[index]\n",
    "\n",
    "df['correct_option_text'] = df.apply(extract_correct_option, axis=1)\n",
    "\n",
    "# Elabora in batch gli embedding delle opzioni corrette\n",
    "correct_texts = df['correct_option_text'].tolist()\n",
    "correct_embeddings = model.encode(correct_texts, device=device, convert_to_tensor=True)\n",
    "\n",
    "# Funzione per verificare in una riga se almeno un segmento del path è semanticamente simile all'opzione corretta\n",
    "def check_similar_option_batch(idx, row):\n",
    "    segments = row['path_segments']\n",
    "    # Se non ci sono segmenti, ritorna False\n",
    "    if not segments:\n",
    "        return False\n",
    "    # Calcola in batch gli embedding dei segmenti (convert_to_tensor=True per operare su GPU)\n",
    "    segments_embeddings = model.encode(segments, device=device, convert_to_tensor=True)\n",
    "    # Recupera l'embedding dell'opzione corretta per la riga corrente\n",
    "    correct_emb = correct_embeddings[idx]\n",
    "    # Normalizza gli embedding per applicare correttamente il calcolo della similarità coseno\n",
    "    correct_norm = correct_emb / correct_emb.norm()\n",
    "    segments_norm = segments_embeddings / segments_embeddings.norm(dim=1, keepdim=True)\n",
    "    # Calcola la similarità coseno tra l'opzione e ogni segmento\n",
    "    similarities = torch.matmul(segments_norm, correct_norm)\n",
    "    # Se almeno una similarità supera la soglia, ritorna True\n",
    "    return (similarities >= SIMILARITY_THRESHOLD).any().item()\n",
    "\n",
    "# Processa ogni riga e determina se il match semantico supera la soglia\n",
    "results = [check_similar_option_batch(idx, row) for idx, row in tqdm(df.iterrows())]\n",
    "df['is_similar_in_path'] = results\n",
    "\n",
    "# Calcola e stampa il contatore finale\n",
    "counter = sum(results)\n",
    "print(f\"Numero di righe in cui almeno un segmento del path risulta simile (>= 80%) all'opzione corretta: {counter}\")\n",
    "\n",
    "# Visualizza il DataFrame aggiornato\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
