{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import ast\n",
    "from together import Together\n",
    "from alive_progress import alive_bar\n",
    "from collections import Counter\n",
    "\n",
    "from prompt import *\n",
    "from classes.utils import extract_option\n",
    "\n",
    "PATH = \"/home/cc/PHD/HealthBranches/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_pro/dataset_updated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for c in df['correct_option'].values:\n",
    "    i =i+1\n",
    "    if isinstance(c, str) and len(c.strip()) != 1:\n",
    "        print(i)\n",
    "        print(df.loc[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{PATH}results/results_quiz_Llama-3.3-70B-Instruct-Turbo-Free.csv\")\n",
    "df[\"zero_shot_new\"] = df[\"zero_shot\"].apply(extract_option)\n",
    "\n",
    "big_filtered = df[df[\"zero_shot_new\"].str.upper() != df[\"real\"].str.upper()]\n",
    "len(big_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = pd.read_csv(f\"{PATH}results/results_quiz_mistral.csv\")\n",
    "small_df = small_df[small_df[\"question\"].isin(big_filtered[\"question\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df[\"zero_shot_new\"] = small_df[\"zero_shot\"].apply(extract_option)\n",
    "small_filtered = small_df[small_df[\"zero_shot_new\"].str.upper() != small_df[\"real\"].str.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory contenente i file CSV\n",
    "csv_directory = \"/home/cc/PHD/HealthBranches/quiz-ultimate-dataset\"\n",
    "\n",
    "# Leggere tutti i CSV\n",
    "csv_files = [os.path.join(csv_directory, f) for f in os.listdir(csv_directory) if f.endswith(\".csv\")]\n",
    "questions = []\n",
    "\n",
    "print(csv_files)\n",
    "\n",
    "# Elaborazione di ogni CSV\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Rimuove la colonna \"zero_shot_rag\" se esiste\n",
    "    if \"zero_shot_rag\" in df.columns:\n",
    "        df = df.drop(columns=[\"zero_shot_rag\"])\n",
    "\n",
    "    # Applica la funzione di estrazione su zero_shot\n",
    "    df[\"zero_shot\"] = df[\"zero_shot\"].apply(extract_option)\n",
    "\n",
    "    # Filtra le righe in cui zero_shot Ã¨ diverso da real\n",
    "    df_filtered = df[df[\"zero_shot\"] != df[\"real\"]].copy()\n",
    "\n",
    "    questions.extend(df_filtered['question'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conta le occorrenze\n",
    "conteggio = Counter(questions)\n",
    "\n",
    "# Ordina in ordine decrescente\n",
    "dizionario_ordinato = dict(sorted(conteggio.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "filtered_dict = {k: v for k, v in dizionario_ordinato.items() if v >= 6}\n",
    "\n",
    "print(len(filtered_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_to_check = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_to_check.csv\")\n",
    "\n",
    "# Trova le domande che sono sia nel dizionario che nel DataFrame\n",
    "matching_questions = [q for q in filtered_dict.keys() if q in questions_to_check[\"question\"].values]\n",
    "print(len(matching_questions))\n",
    "\n",
    "print(\"Domande trovate nel DataFrame:\", len(matching_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il CSV (sostituisci 'file.csv' con il tuo file)\n",
    "df = pd.read_csv(\"/home/cc/PHD/HealthBranches/results/results_checker_70B.csv\")\n",
    "\n",
    "# Conta le occorrenze di ogni valore nella colonna \"check\"\n",
    "conteggio = df[\"check\"].value_counts().sort_index()\n",
    "\n",
    "# Stampa il risultato\n",
    "print(conteggio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from typing import List\n",
    "\n",
    "class LLMinference:\n",
    "    def __init__(self, llm_name, temperature=0.0, num_predict=128):\n",
    "        self.llm_name = llm_name\n",
    "        self.model = OllamaLLM(model=llm_name, temperature=temperature, num_predict=num_predict,\n",
    "                               system=\"Answer the question directly without including any reasoning or explanation. Do not use <think> tags or any analytical text.\") \n",
    "\n",
    "    def _transform_query(self, query: str) -> str:\n",
    "        return f'Represent this sentence for searching relevant passages: {query}'\n",
    "\n",
    "    def single_inference(self, query: str, template: str, path: str, text: str,  choices: List[str], cond: str,  context) -> str | List[str]:\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in context])\n",
    "        prompt_template = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        if choices: # quiz\n",
    "            if path != \"\" and text != \"\":\n",
    "                prompt = prompt_template.format(context=context_text, question=query, path=path, text=text, condition=cond, o1=choices[0], o2=choices[1], o3=choices[2], o4=choices[3], o5=choices[4])\n",
    "            else:\n",
    "                prompt = prompt_template.format(context=context_text, question=query, condition=cond, o1=choices[0], o2=choices[1], o3=choices[2], o4=choices[3], o5=choices[4])\n",
    "        else: # open question\n",
    "            if path != \"\" and text != \"\":\n",
    "                prompt = prompt_template.format(context=context_text, question=query, path=path, text=text, condition=cond)\n",
    "            else:\n",
    "                prompt = prompt_template.format(context=context_text, question=query, condition=cond)\n",
    "\n",
    "        response_text = self.model.invoke(prompt)\n",
    "        response_text = response_text.strip().replace(\"\\n\", \"\").replace(\"  \", \"\")\n",
    "\n",
    "        sources = [doc.metadata.get(\"source\", None) for doc in context]\n",
    "        \n",
    "        return response_text, sources\n",
    "\n",
    "    def qea_evaluation(self, query: str, template: str, path: str, txt: str, choices: List[str], cond: str, vector_store, k: int = 3) -> str:\n",
    "\n",
    "        results = vector_store.search(query=query, k=3)\n",
    "\n",
    "        response, sources = self.single_inference(query, template, path, txt, choices, cond, results)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def inf(self, string):\n",
    "        response_text = self.model.invoke(string)\n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep = LLMinference(\"deepseek-r1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep.inf(\"Question: what is the capital of germany? \\n Answer: \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
