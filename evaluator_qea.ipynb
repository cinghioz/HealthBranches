{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from classes.utils import check_options, extract_option\n",
    "\n",
    "BENCH =  \"/home/cc/PHD/HealthBranches/results/results_QUIZ_bench_\"\n",
    "BASE =  \"/home/cc/PHD/HealthBranches/results/results_QUIZ_baseline_\"\n",
    "MEDQA =  \"/home/cc/PHD/HealthBranches/results-medqa/results_QUIZ_medqa_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_q = pd.read_csv(\"/home/cc/PHD/HealthBranches/question_checked.csv\")\n",
    "\n",
    "correct_q['label'].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_line(df_old, df_new):\n",
    "    # Itera sulle righe del secondo CSV\n",
    "    for _, row in df_new.iterrows():\n",
    "        idx = row[\"ID\"]  # ID nel secondo CSV che corrisponde all'indice del primo CSV\n",
    "        label = row[\"label\"]\n",
    "        \n",
    "        if label == \"1\":\n",
    "            df_old.at[idx, \"correct_option\"] = row[\"correct_option\"]\n",
    "        elif label == \"2\":\n",
    "            df_old.at[idx, \"question\"] = row[\"question\"]\n",
    "\n",
    "    return df_old\n",
    "\n",
    "def evaluate_answers(file_path, model):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # to_remove = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_to_check.csv\")\n",
    "    # correct_q = pd.read_csv(\"/home/cc/PHD/HealthBranches/question_checked.csv\")\n",
    "    \n",
    "    # matching_indices = df[df[\"question\"].isin(to_remove[\"question\"])].index\n",
    "\n",
    "    # to_remove = change_line(to_remove, correct_q)\n",
    "    # df.loc[matching_indices, [\"question\", \"real\"]] = to_remove[[\"question\", \"correct_option\"]].values  # Va solo per bench\n",
    "\n",
    "    # to_remove = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_to_check.csv\")\n",
    "    # df = df[~df[\"question\"].isin(to_remove[\"question\"])]\n",
    "\n",
    "    df['correct_answer'] = df['real']\n",
    "    accs = []\n",
    "    \n",
    "    # Evaluate all columns that start with \"zero_shot\" or \"one_shot\"\n",
    "    for col in [c for c in df.columns if c.startswith((\"zero_shot\", \"one_shot\"))]:\n",
    "        df[f'{col}_choice'] = df[col].apply(extract_option)\n",
    "        df[f'{col}_is_correct'] = df[f'{col}_choice'] == df['correct_answer']\n",
    "        accuracy = df[f'{col}_is_correct'].mean()\n",
    "        print(f'Accuracy for {col}: {accuracy:.2%}')\n",
    "        accs.append(accuracy)\n",
    "    \n",
    "    accs.insert(0, model)\n",
    "\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_option(\"The correct answer is A.Given the presence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, nonelist = check_options(pd.read_csv(f\"/home/cc/PHD/HealthBranches/results/results_quiz_baseline_mistral.csv\"))\n",
    "# print(len(nonelist))\n",
    "\n",
    "# nonelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"mistral:7b\", \"gemma:7b\", \"gemma2:9b\", \"gemma3:4b\", \"llama3.1:8b\", \"qwen2.5:7b\", \"phi4-mini:3.8b\", \"llama2:7b\"]\n",
    "\n",
    "bench = [evaluate_answers(f\"{BENCH}{model}.csv\", model) for model in models]\n",
    "baseline = [evaluate_answers(f\"{BASE}{model}.csv\", model) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### BASELINE ###\")\n",
    "for model in models:\n",
    "    print(f\"Results for {model}\")\n",
    "    evaluate_answers(f\"{BASE}{model}.csv\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### BENCH ###\")\n",
    "for model in models:\n",
    "    print(f\"Results for {model}\")\n",
    "    evaluate_answers(f\"{BENCH}{model}.csv\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BENCH all q ###\n",
    "Results for mistral:7b\n",
    "Accuracy for zero_shot: 47.36%\n",
    "Accuracy for zero_shot_rag: 56.01%\n",
    "Results for llama3.1:8b\n",
    "Accuracy for zero_shot: 59.98%\n",
    "Accuracy for zero_shot_rag: 62.04%\n",
    "Results for llama2:7b\n",
    "Accuracy for zero_shot: 20.22%\n",
    "Accuracy for zero_shot_rag: 28.68%\n",
    "Results for gemma:7b\n",
    "Accuracy for zero_shot: 52.86%\n",
    "Accuracy for zero_shot_rag: 57.96%\n",
    "Results for gemma2:9b\n",
    "Accuracy for zero_shot: 60.83%\n",
    "Accuracy for zero_shot_rag: 65.61%\n",
    "Results for qwen2.5:7b\n",
    "Accuracy for zero_shot: 60.56%\n",
    "Accuracy for zero_shot_rag: 62.92%\n",
    "Results for phi4-mini:3.8b\n",
    "Accuracy for zero_shot: 58.61%\n",
    "Accuracy for zero_shot_rag: 65.05%\n",
    "Results for gemma3:4b\n",
    "Accuracy for zero_shot: 55.65%\n",
    "Accuracy for zero_shot_rag: 57.21%\n",
    "Results for llama3.3:70b\n",
    "Accuracy for zero_shot: 64.50%\n",
    "Accuracy for zero_shot_rag: 67.63%\n",
    "\n",
    "### BENCH correct q ###\n",
    "Results for mistral:7b\n",
    "Accuracy for zero_shot: 47.10%\n",
    "Accuracy for zero_shot_rag: 55.70%\n",
    "Results for llama3.1:8b\n",
    "Accuracy for zero_shot: 60.15%\n",
    "Accuracy for zero_shot_rag: 61.77%\n",
    "Results for llama2:7b\n",
    "Accuracy for zero_shot: 20.12%\n",
    "Accuracy for zero_shot_rag: 28.77%\n",
    "Results for gemma:7b\n",
    "Accuracy for zero_shot: 52.37%\n",
    "Accuracy for zero_shot_rag: 57.16%\n",
    "Results for gemma2:9b\n",
    "Accuracy for zero_shot: 61.36%\n",
    "Accuracy for zero_shot_rag: 65.39%\n",
    "Results for qwen2.5:7b\n",
    "Accuracy for zero_shot: 60.58%\n",
    "Accuracy for zero_shot_rag: 62.48%\n",
    "Results for phi4-mini:3.8b\n",
    "Accuracy for zero_shot: 58.61%\n",
    "Accuracy for zero_shot_rag: 64.86%\n",
    "Results for gemma3:4b\n",
    "Accuracy for zero_shot: 55.72%\n",
    "Accuracy for zero_shot_rag: 56.87%\n",
    "Results for llama3.3:70b\n",
    "Accuracy for zero_shot: 65.71%\n",
    "Accuracy for zero_shot_rag: 68.00%\n",
    "\n",
    "### BENCH no q ###\n",
    "Results for mistral:7b\n",
    "Accuracy for zero_shot: 58.79%\n",
    "Accuracy for zero_shot_rag: 69.44%\n",
    "Results for llama3.1:8b\n",
    "Accuracy for zero_shot: 77.44%\n",
    "Accuracy for zero_shot_rag: 78.78%\n",
    "Results for llama2:7b\n",
    "Accuracy for zero_shot: 20.74%\n",
    "Accuracy for zero_shot_rag: 32.18%\n",
    "Results for gemma:7b\n",
    "Accuracy for zero_shot: 64.56%\n",
    "Accuracy for zero_shot_rag: 70.57%\n",
    "Results for gemma2:9b\n",
    "Accuracy for zero_shot: 79.43%\n",
    "Accuracy for zero_shot_rag: 81.94%\n",
    "Results for qwen2.5:7b\n",
    "Accuracy for zero_shot: 77.78%\n",
    "Accuracy for zero_shot_rag: 78.57%\n",
    "Results for phi4-mini:3.8b\n",
    "Accuracy for zero_shot: 74.90%\n",
    "Accuracy for zero_shot_rag: 80.53%\n",
    "Results for gemma3:4b\n",
    "Accuracy for zero_shot: 69.57%\n",
    "Accuracy for zero_shot_rag: 70.36%\n",
    "Results for llama3.3:70b\n",
    "Accuracy for zero_shot: 90.90%\n",
    "Accuracy for zero_shot_rag: 88.53%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### MEDQA ###\")\n",
    "for model in models:\n",
    "    print(f\"Results for {model}\")\n",
    "    evaluate_answers(f\"{MEDQA}{model}.csv\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_chart(bench, baseline, bar1, bar2, bar3, xl, yl, title):\n",
    "    # Creiamo un dizionario dalla lista baseline per una ricerca veloce\n",
    "    baseline_dict = {item[0]: item[1] for item in baseline}\n",
    "\n",
    "    # Uniamo le liste\n",
    "    merged_list = [item + [baseline_dict[item[0]]] for item in bench if item[0] in baseline_dict]\n",
    "\n",
    "    # Ordiniamo la lista in base al primo float (item[1])\n",
    "    merged_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(merged_list)\n",
    "\n",
    "    # Estrai le etichette e i valori\n",
    "    labels = [x[0] for x in merged_list]\n",
    "    values1 = [x[1] for x in merged_list]\n",
    "    values2 = [x[2] for x in merged_list]\n",
    "    values3 = [x[3] for x in merged_list]\n",
    "\n",
    "    # Imposta la posizione delle barre con pi√π spazio tra i gruppi\n",
    "    x = np.arange(len(labels)) * 1.3  # Moltiplica per aumentare la distanza tra i gruppi\n",
    "    width = 0.35  # Larghezza delle barre\n",
    "\n",
    "    # Aumenta le dimensioni del grafico\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    bars1 = ax.bar(x - width, values1, width, label=bar1)\n",
    "    bars2 = ax.bar(x, values2, width, label=bar2)\n",
    "    bars3 = ax.bar(x + width, values3, width, label=bar3)    \n",
    "\n",
    "    # Etichette e titolo\n",
    "    ax.set_xlabel(xl)\n",
    "    ax.set_ylabel(yl)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    # Mostra il grafico\n",
    "    plt.show()\n",
    "\n",
    "show_chart(bench, baseline, \"no RAG\", \"RAG\", \"Baseline\", \"Models\", \"Accuracy\", \"Benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import random\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def balance_correct_answer(csv_file, output_file):\n",
    "    # Carica il file CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Definisce le lettere per le opzioni (A, B, C, D, E)\n",
    "    option_letters = list(string.ascii_uppercase[:5])\n",
    "    \n",
    "    # Conta la distribuzione attuale delle risposte corrette\n",
    "    correct_counts = Counter(df['correct_option'])\n",
    "    \n",
    "    # Calcola il numero desiderato per ciascuna lettera\n",
    "    target_count = len(df) // 5\n",
    "    \n",
    "    # Inizializza un dizionario per tenere traccia delle assegnazioni\n",
    "    assigned_counts = {letter: 0 for letter in option_letters}\n",
    "    \n",
    "    # Funzione per ribilanciare la posizione della risposta corretta\n",
    "    def rebalance(row):\n",
    "        options = ast.literal_eval(row['options'].replace(\"['\", '[\"').replace(\"']\", '\"]').replace(\"', '\", '\", \"'))  # Converte la stringa in lista\n",
    "        correct_letter = row['correct_option']\n",
    "        correct_index = option_letters.index(correct_letter)\n",
    "        correct_answer = options[correct_index]\n",
    "        \n",
    "        # Trova le lettere meno usate\n",
    "        available_letters = [letter for letter in option_letters if assigned_counts[letter] < target_count]\n",
    "        \n",
    "        # Se tutte le lettere sono bilanciate, assegna a caso\n",
    "        new_correct_letter = random.choice(available_letters) if available_letters else random.choice(option_letters)\n",
    "        new_correct_index = option_letters.index(new_correct_letter)\n",
    "        \n",
    "        # Rimescola le risposte\n",
    "        random.shuffle(options)\n",
    "        \n",
    "        # Sposta la risposta corretta nella nuova posizione\n",
    "        options.remove(correct_answer)\n",
    "        options.insert(new_correct_index, correct_answer)\n",
    "        \n",
    "        # Aggiorna il conteggio\n",
    "        assigned_counts[new_correct_letter] += 1\n",
    "        \n",
    "        return pd.Series([str(options), new_correct_letter])\n",
    "    \n",
    "    # Applica la funzione a ogni riga\n",
    "    df[['options', 'correct_option']] = df.apply(rebalance, axis=1)\n",
    "    \n",
    "    # Salva il nuovo file CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full.csv\")\n",
    "print(\"Distribuzione prima:\")\n",
    "print(Counter(df[\"correct_option\"]))\n",
    "\n",
    "# # Esegui la funzione su un file di esempio\n",
    "# bal_df = balance_correct_answer(\"/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full.csv\", \"/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full_balanced.csv\")\n",
    "\n",
    "# # df = balance_correct_options(df)\n",
    "# print(\"Distribuzione dopo:\")\n",
    "# print(Counter(bal_df[\"correct_option\"]))\n",
    "\n",
    "# df.to_csv(\"/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full_balanced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica i CSV\n",
    "df_dataset = pd.read_csv('/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full_balanced.csv')       # CSV completo delle domande\n",
    "df_subset = pd.read_csv('/home/cc/PHD/HealthBranches/questions_to_check.csv')         # CSV con le domande da correggere\n",
    "df_corrected = pd.read_csv('/home/cc/PHD/HealthBranches/question_checked.csv')    # CSV con le domande corrette; contiene la colonna \"ID\" (l'indice nel CSV subset)\n",
    "\n",
    "# Resetta l'indice di df_subset per garantire che sia numerato da 0 a len(df_subset)-1\n",
    "# df_subset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "mapping = {}\n",
    "for _, row in df_corrected.iterrows():\n",
    "    idx = int(row['ID'])\n",
    "    if idx < len(df_subset):\n",
    "        original_question = df_subset.loc[idx, 'question']\n",
    "        mapping[original_question] = {\n",
    "            'question': row['question'],\n",
    "            'answer': row['answer'],\n",
    "            'options': row['options'],\n",
    "            'correct_option': row['correct_option'],\n",
    "            'path': row['path']\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Warning: ID {idx} non presente in df_subset\")\n",
    "\n",
    "# Funzione per aggiornare una riga del dataset, se la domanda √® presente nel mapping\n",
    "def update_row(row):\n",
    "    corrections = mapping.get(row['question'])\n",
    "    if corrections:\n",
    "        row['question'] = corrections['question']\n",
    "        row['answer'] = corrections['answer']\n",
    "        row['options'] = corrections['options']\n",
    "        row['correct_option'] = corrections['correct_option']\n",
    "        row['path'] = corrections['path']\n",
    "    return row\n",
    "\n",
    "# Applica la funzione a ogni riga del dataset\n",
    "df_dataset = df_dataset.apply(update_row, axis=1)\n",
    "df_dataset.drop(columns=['answer'], inplace=True)\n",
    "\n",
    "# Salva il dataset aggiornato\n",
    "df_dataset.to_csv('dataset_updated.csv', index=False)\n",
    "print(\"Dataset aggiornato salvato come 'dataset_updated.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
