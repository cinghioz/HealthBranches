{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from classes.utils import check_options, extract_option\n",
    "\n",
    "BENCH =  \"/home/cc/PHD/HealthBranches/results/results_QUIZ_bench_\"\n",
    "BASE =  \"/home/cc/PHD/HealthBranches/results/results_QUIZ_baseline_\"\n",
    "MEDQA =  \"/home/cc/PHD/HealthBranches/results-medqa/results_QUIZ_medqa_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_line(df_old, df_new):\n",
    "    # Itera sulle righe del secondo CSV\n",
    "    for _, row in df_new.iterrows():\n",
    "        idx = row[\"ID\"]  # ID nel secondo CSV che corrisponde all'indice del primo CSV\n",
    "        label = row[\"label\"]\n",
    "        \n",
    "        if label == \"1\":\n",
    "            df_old.at[idx, \"correct_option\"] = row[\"correct_option\"]\n",
    "        elif label == \"2\":\n",
    "            df_old.at[idx, \"question\"] = row[\"question\"]\n",
    "\n",
    "    return df_old\n",
    "\n",
    "def evaluate_answers(file_path, model):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # subset_df = pd.read_csv(\"/home/cc/PHD/HealthBranches/question_checked.csv\")\n",
    "\n",
    "    #df = df[~df['question'].isin(subset_df['question'])] # senza le checked\n",
    "    # df = df[df['question'].isin(subset_df['question'])] # solo checked\n",
    "\n",
    "    df['correct_answer'] = df['real']\n",
    "    accs = []\n",
    "\n",
    "    for col in [c for c in df.columns if c.startswith((\"zero_shot\", \"one_shot\"))]:\n",
    "        df[f'{col}_choice'] = df[col].apply(extract_option)\n",
    "        df[f'{col}_is_correct'] = df[f'{col}_choice'] == df['correct_answer']\n",
    "        accuracy = df[f'{col}_is_correct'].mean()\n",
    "        print(f'Accuracy for {col}: {accuracy:.2%}')\n",
    "        accs.append(accuracy)\n",
    "    \n",
    "    accs.insert(0, model)\n",
    "\n",
    "    return accs\n",
    "\n",
    "def evaluate_answers_by_cond(file_path, model, condition_value=None):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Filter the DataFrame for the specific condition if provided.\n",
    "    if condition_value is not None:\n",
    "        df = df[df['name'] == condition_value]\n",
    "    \n",
    "    # Set up the correct answer column.\n",
    "    df['correct_answer'] = df['real']\n",
    "    accs = []\n",
    "\n",
    "    # Loop through each relevant column (those starting with \"zero_shot\" or \"one_shot\").\n",
    "    for col in [c for c in df.columns if c.startswith((\"zero_shot\", \"one_shot\"))]:\n",
    "        df[f'{col}_choice'] = df[col].apply(extract_option)\n",
    "        df[f'{col}_is_correct'] = df[f'{col}_choice'] == df['correct_answer']\n",
    "        accuracy = df[f'{col}_is_correct'].mean()\n",
    "        # print(f'Accuracy for {col} under condition  == {condition_value}: {accuracy:.2%}')\n",
    "        if accuracy < 0.5:\n",
    "             print(f'Accuracy for {col} under condition  == {condition_value}: {accuracy:.2%}')\n",
    "        accs.append(accuracy)\n",
    "    \n",
    "    accs.insert(0, model)\n",
    "    return accs\n",
    "\n",
    "def find_incorrect_indices(file_path, model):\n",
    "    df = pd.read_csv(file_path)\n",
    "    subset_df = pd.read_csv(\"/home/cc/PHD/HealthBranches/question_checked.csv\")\n",
    "\n",
    "    #df = df[~df['question'].isin(subset_df['question'])] # senza le checked\n",
    "    # df = df[df['question'].isin(subset_df['question'])] # solo checked\n",
    "    print(f\"dataframe shape: {df.shape}, model: {model}\")\n",
    "\n",
    "    incorrect_indices = df.index[df[\"zero_shot\"].apply(extract_option) != df[\"real\"]].tolist()\n",
    "    return incorrect_indices\n",
    "\n",
    "def find_common_wrongs(lists_of_indices):\n",
    "    if not lists_of_indices:\n",
    "        return []\n",
    "    \n",
    "    # Start with the set of indices from the first list\n",
    "    common = set(lists_of_indices[0])\n",
    "    \n",
    "    # Intersect with the indices from each subsequent list\n",
    "    for indices in lists_of_indices[1:]:\n",
    "        common &= set(indices)\n",
    "    \n",
    "    # Return the sorted list of common indices\n",
    "    return sorted(common)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/cc/PHD/HealthBranches/results/results_QUIZ_bench_DeepSeek-R1-Distill-Qwen-7B.csv\")\n",
    "for row in df.iterrows():\n",
    "    if extract_option(df[\"zero_shot_rag\"]) == None:\n",
    "        print(df[\"zero_shot\"])\n",
    "\n",
    "evaluate_answers(BENCH + \"DeepSeek-R1-Distill-Qwen-7B.csv\", \"DeepSeek-R1-Distill-Qwen-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"mistral:7b\", \"gemma:7b\", \"gemma2:9b\", \"gemma3:4b\", \"llama3.1:8b\", \"qwen2.5:7b\", \n",
    "          \"phi4:14b\", \"llama2:7b\", \"Llama-3.3-70B-Instruct-Turbo-Free\"]\n",
    "bench = [evaluate_answers(f\"{BENCH}{model}.csv\", model) for model in models]\n",
    "baseline = [evaluate_answers(f\"{BASE}{model}.csv\", model) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for zero_shot under condition  == noncardiac chest pain: 37.50%\n",
      "Accuracy for zero_shot_rag under condition  == noncardiac chest pain: 25.00%\n",
      "Accuracy for zero_shot_rag under condition  == adrenal incidentaloma: 40.00%\n",
      "Accuracy for zero_shot under condition  == hyperkalemia: 0.00%\n",
      "Accuracy for zero_shot_rag under condition  == hyperkalemia: 0.00%\n",
      "Accuracy for zero_shot under condition  == snake venom poisoning: 33.33%\n",
      "Accuracy for zero_shot_rag under condition  == belching: 33.33%\n",
      "Accuracy for zero_shot under condition  == biliary colic: 35.71%\n",
      "Accuracy for zero_shot_rag under condition  == thyroid pain: 25.00%\n",
      "Accuracy for zero_shot under condition  == foreign body ingestion: 45.45%\n",
      "Accuracy for zero_shot under condition  == ectopic pregnancy: 44.44%\n",
      "Accuracy for zero_shot under condition  == tests of thyroid function: 40.00%\n",
      "Accuracy for zero_shot_rag under condition  == tests of thyroid function: 40.00%\n",
      "Accuracy for zero_shot under condition  == chronic myelogenous leukemia: 42.86%\n",
      "Accuracy for zero_shot_rag under condition  == chronic myelogenous leukemia: 42.86%\n",
      "Accuracy for zero_shot_rag under condition  == hypotension: 40.00%\n",
      "Accuracy for zero_shot under condition  == dilated pupil: 0.00%\n",
      "Accuracy for zero_shot_rag under condition  == dilated pupil: 0.00%\n",
      "Accuracy for zero_shot under condition  == hyperglycemia: 40.00%\n",
      "Accuracy for zero_shot under condition  == antimicrobial prophylaxis in surgical patients: 22.22%\n",
      "Accuracy for zero_shot under condition  == peripheral neuropathy: 42.86%\n",
      "Accuracy for zero_shot under condition  == respiratory symptoms in hiv-infected patients: 40.91%\n",
      "Accuracy for zero_shot_rag under condition  == respiratory symptoms in hiv-infected patients: 47.16%\n",
      "Accuracy for zero_shot under condition  == unstable angina: 27.27%\n",
      "Accuracy for zero_shot under condition  == gynecomastia: 48.28%\n",
      "Accuracy for zero_shot_rag under condition  == hypermagnesemia: 0.00%\n",
      "Accuracy for zero_shot under condition  == palpitations: 33.33%\n",
      "Accuracy for zero_shot_rag under condition  == palpitations: 33.33%\n",
      "Accuracy for zero_shot_rag under condition  == urticaria: 25.00%\n",
      "Accuracy for zero_shot_rag under condition  == metabolic alkalosis: 40.00%\n",
      "Accuracy for zero_shot_rag under condition  == persistent excessive sweating: 42.86%\n",
      "Accuracy for zero_shot under condition  == transfusion therapy red blood cells: 20.00%\n",
      "Accuracy for zero_shot_rag under condition  == status epilepticus: 46.15%\n",
      "Accuracy for zero_shot_rag under condition  == selection of patients for transplantation: 0.00%\n",
      "Accuracy for zero_shot under condition  == positive tuberculin skin test (ppd): 20.00%\n",
      "Accuracy for zero_shot_rag under condition  == positive tuberculin skin test (ppd): 0.00%\n",
      "Accuracy for zero_shot under condition  == staphylococcus aureus bacteremia: 20.00%\n",
      "Accuracy for zero_shot_rag under condition  == staphylococcus aureus bacteremia: 40.00%\n"
     ]
    }
   ],
   "source": [
    "conditions = pd.read_csv(\"/home/cc/PHD/HealthBranches/results/results_QUIZ_baseline_Llama-3.3-70B-Instruct-Turbo-Free.csv\")['name'].unique().tolist()\n",
    "\n",
    "for condition in conditions:\n",
    "    evaluate_answers_by_cond(f\"{BASE}Llama-3.3-70B-Instruct-Turbo-Free.csv\", \"Llama-3.3-70B-Instruct-Turbo-Free\", condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"mistral:7b\", \"gemma:7b\", \"gemma2:9b\", \"llama3.1:8b\", \"qwen2.5:7b\", \"phi4:14b\"]\n",
    "wrongs = [find_incorrect_indices(f\"{BASE}{model}.csv\", model) for model in models]\n",
    "w2 = [find_incorrect_indices(f\"{BENCH}{model}.csv\", model) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(find_common_wrongs(wrongs+w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{BASE}{\"gemma2:9b\"}.csv\")\n",
    "subset_df = pd.read_csv(\"/home/cc/PHD/HealthBranches/question_checked.csv\")\n",
    "\n",
    "#df = df[~df['question'].isin(subset_df['question'])] # senza le checked\n",
    "df = df[df['question'].isin(subset_df['question'])] # solo checked\n",
    "filtered_df = df.loc[find_common_wrongs(wrongs+w2)]\n",
    "print(f\"filtered_df shape: {filtered_df.shape}\")\n",
    "\n",
    "# Save the filtered dataframe to a CSV file without writing the index column\n",
    "# filtered_df.to_csv(\"/home/cc/PHD/HealthBranches/questions_to_check_BASE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### BASELINE ###\")\n",
    "for model in models:\n",
    "    print(f\"Results for {model}\")\n",
    "    evaluate_answers(f\"{BASE}{model}.csv\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### BENCH ###\")\n",
    "for model in models:\n",
    "    print(f\"Results for {model}\")\n",
    "    evaluate_answers(f\"{BENCH}{model}.csv\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### MEDQA ###\")\n",
    "for model in models:\n",
    "    print(f\"Results for {model}\")\n",
    "    evaluate_answers(f\"{MEDQA}{model}.csv\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_chart(bench, baseline, bar1, bar2, bar3, xl, yl, title):\n",
    "    # Creiamo un dizionario dalla lista baseline per una ricerca veloce\n",
    "    baseline_dict = {item[0]: item[1] for item in baseline}\n",
    "\n",
    "    # Uniamo le liste\n",
    "    merged_list = [item + [baseline_dict[item[0]]] for item in bench if item[0] in baseline_dict]\n",
    "\n",
    "    # Ordiniamo la lista in base al primo float (item[1])\n",
    "    merged_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(merged_list)\n",
    "\n",
    "    # Estrai le etichette e i valori\n",
    "    labels = [x[0] for x in merged_list]\n",
    "    values1 = [x[1] for x in merged_list]\n",
    "    values2 = [x[2] for x in merged_list]\n",
    "    values3 = [x[3] for x in merged_list]\n",
    "\n",
    "    # Imposta la posizione delle barre con più spazio tra i gruppi\n",
    "    x = np.arange(len(labels)) * 1.3  # Moltiplica per aumentare la distanza tra i gruppi\n",
    "    width = 0.35  # Larghezza delle barre\n",
    "\n",
    "    # Aumenta le dimensioni del grafico\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    bars1 = ax.bar(x - width, values1, width, label=bar1)\n",
    "    bars2 = ax.bar(x, values2, width, label=bar2)\n",
    "    bars3 = ax.bar(x + width, values3, width, label=bar3)    \n",
    "\n",
    "    # Etichette e titolo\n",
    "    ax.set_xlabel(xl)\n",
    "    ax.set_ylabel(yl)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    # Mostra il grafico\n",
    "    plt.show()\n",
    "\n",
    "show_chart(bench, baseline, \"no RAG\", \"RAG\", \"Baseline\", \"Models\", \"Accuracy\", \"Benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import random\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def balance_correct_answer(csv_file, output_file):\n",
    "    # Carica il file CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Definisce le lettere per le opzioni (A, B, C, D, E)\n",
    "    option_letters = list(string.ascii_uppercase[:5])\n",
    "    \n",
    "    # Conta la distribuzione attuale delle risposte corrette\n",
    "    correct_counts = Counter(df['correct_option'])\n",
    "    \n",
    "    # Calcola il numero desiderato per ciascuna lettera\n",
    "    target_count = len(df) // 5\n",
    "    \n",
    "    # Inizializza un dizionario per tenere traccia delle assegnazioni\n",
    "    assigned_counts = {letter: 0 for letter in option_letters}\n",
    "    \n",
    "    # Funzione per ribilanciare la posizione della risposta corretta\n",
    "    def rebalance(row):\n",
    "        options = ast.literal_eval(row['options'].replace(\"['\", '[\"').replace(\"']\", '\"]').replace(\"', '\", '\", \"'))  # Converte la stringa in lista\n",
    "        correct_letter = row['correct_option']\n",
    "        correct_index = option_letters.index(correct_letter)\n",
    "        correct_answer = options[correct_index]\n",
    "        \n",
    "        # Trova le lettere meno usate\n",
    "        available_letters = [letter for letter in option_letters if assigned_counts[letter] < target_count]\n",
    "        \n",
    "        # Se tutte le lettere sono bilanciate, assegna a caso\n",
    "        new_correct_letter = random.choice(available_letters) if available_letters else random.choice(option_letters)\n",
    "        new_correct_index = option_letters.index(new_correct_letter)\n",
    "        \n",
    "        # Rimescola le risposte\n",
    "        random.shuffle(options)\n",
    "        \n",
    "        # Sposta la risposta corretta nella nuova posizione\n",
    "        options.remove(correct_answer)\n",
    "        options.insert(new_correct_index, correct_answer)\n",
    "        \n",
    "        # Aggiorna il conteggio\n",
    "        assigned_counts[new_correct_letter] += 1\n",
    "        \n",
    "        return pd.Series([str(options), new_correct_letter])\n",
    "    \n",
    "    # Applica la funzione a ogni riga\n",
    "    df[['options', 'correct_option']] = df.apply(rebalance, axis=1)\n",
    "    \n",
    "    # Salva il nuovo file CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full.csv\")\n",
    "print(\"Distribuzione prima:\")\n",
    "print(Counter(df[\"correct_option\"]))\n",
    "\n",
    "# # Esegui la funzione su un file di esempio\n",
    "# bal_df = balance_correct_answer(\"/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full.csv\", \"/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full_balanced.csv\")\n",
    "\n",
    "# # df = balance_correct_options(df)\n",
    "# print(\"Distribuzione dopo:\")\n",
    "# print(Counter(bal_df[\"correct_option\"]))\n",
    "\n",
    "# df.to_csv(\"/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full_balanced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica i CSV\n",
    "df_dataset = pd.read_csv('/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full_balanced.csv')       # CSV completo delle domande\n",
    "df_subset = pd.read_csv('/home/cc/PHD/HealthBranches/questions_to_check.csv')         # CSV con le domande da correggere\n",
    "df_corrected = pd.read_csv('/home/cc/PHD/HealthBranches/question_checked.csv')    # CSV con le domande corrette; contiene la colonna \"ID\" (l'indice nel CSV subset)\n",
    "\n",
    "# Resetta l'indice di df_subset per garantire che sia numerato da 0 a len(df_subset)-1\n",
    "# df_subset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "mapping = {}\n",
    "for _, row in df_corrected.iterrows():\n",
    "    idx = int(row['ID'])\n",
    "    if idx < len(df_subset):\n",
    "        original_question = df_subset.loc[idx, 'question']\n",
    "        mapping[original_question] = {\n",
    "            'question': row['question'],\n",
    "            'answer': row['answer'],\n",
    "            'options': row['options'],\n",
    "            'correct_option': row['correct_option'],\n",
    "            'path': row['path']\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Warning: ID {idx} non presente in df_subset\")\n",
    "\n",
    "# Funzione per aggiornare una riga del dataset, se la domanda è presente nel mapping\n",
    "def update_row(row):\n",
    "    corrections = mapping.get(row['question'])\n",
    "    if corrections:\n",
    "        row['question'] = corrections['question']\n",
    "        row['answer'] = corrections['answer']\n",
    "        row['options'] = corrections['options']\n",
    "        row['correct_option'] = corrections['correct_option']\n",
    "        row['path'] = corrections['path']\n",
    "    return row\n",
    "\n",
    "# Applica la funzione a ogni riga del dataset\n",
    "df_dataset = df_dataset.apply(update_row, axis=1)\n",
    "df_dataset.drop(columns=['answer'], inplace=True)\n",
    "\n",
    "# Salva il dataset aggiornato\n",
    "df_dataset.to_csv('dataset_updated.csv', index=False)\n",
    "print(\"Dataset aggiornato salvato come 'dataset_updated.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
