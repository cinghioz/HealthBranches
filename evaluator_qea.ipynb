{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from classes.utils import check_options, extract_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answers(file_path, model):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # to_remove = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_to_check.csv\")\n",
    "\n",
    "    # df = df[~df[\"question\"].isin(to_remove[\"question\"])]\n",
    "\n",
    "    df['correct_answer'] = df['real']\n",
    "    accs = []\n",
    "    \n",
    "    # Evaluate all columns that start with \"zero_shot\" or \"one_shot\"\n",
    "    for col in [c for c in df.columns if c.startswith((\"zero_shot\", \"one_shot\"))]:\n",
    "        df[f'{col}_choice'] = df[col].apply(extract_option)\n",
    "        df[f'{col}_is_correct'] = df[f'{col}_choice'] == df['correct_answer']\n",
    "        accuracy = df[f'{col}_is_correct'].mean()\n",
    "        print(f'Accuracy for {col}: {accuracy:.2%}')\n",
    "        accs.append(accuracy)\n",
    "    \n",
    "    accs.insert(0, model)\n",
    "\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_option(\"The correct answer is A.Given the presence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, nonelist = check_options(pd.read_csv(f\"/home/cc/PHD/HealthBranches/results/results_quiz_baseline_mistral.csv\"))\n",
    "print(len(nonelist))\n",
    "\n",
    "nonelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"mistral\", \"llama3.1:8b\", \"llama2:7b\", \"gemma:7b\", \"gemma2:9b\", \"qwen2.5:7b\", \"phi4:14b\", \"Llama-3.3-70B-Instruct-Turbo-Free\"]\n",
    "\n",
    "bench = [evaluate_answers(f\"/home/cc/PHD/HealthBranches/results/results_quiz_{model}.csv\", model) for model in models]\n",
    "baseline = [evaluate_answers(f\"/home/cc/PHD/HealthBranches/results/results_quiz_baseline_{model}.csv\", model) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_answers(f\"/home/cc/PHD/HealthBranches/results/results_quiz_baseline_Llama-3.3-70B-Instruct-Turbo-Free.csv\", \"405-B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### BASELINE ###\")\n",
    "for model in models:\n",
    "    print(f\"Results for {model}\")\n",
    "    evaluate_answers(f\"/home/cc/PHD/HealthBranches/results/results_quiz_baseline_{model}.csv\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### BENCH ###\")\n",
    "for model in models:\n",
    "    print(f\"Results for {model}\")\n",
    "    evaluate_answers(f\"/home/cc/PHD/HealthBranches/results/results_quiz_{model}.csv\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BENCH ###\n",
    "Results for mistral\n",
    "Accuracy for zero_shot: 54.63%\n",
    "Accuracy for zero_shot_rag: 60.10%\n",
    "Results for llama3.1:8b\n",
    "Accuracy for zero_shot: 59.32%\n",
    "Accuracy for zero_shot_rag: 66.20%\n",
    "Results for llama2:7b\n",
    "Accuracy for zero_shot: 18.91%\n",
    "Accuracy for zero_shot_rag: 43.79%\n",
    "Results for gemma:7b\n",
    "Accuracy for zero_shot: 57.16%\n",
    "Accuracy for zero_shot_rag: 64.11%\n",
    "Results for gemma2:9b\n",
    "Accuracy for zero_shot: 63.45%\n",
    "Accuracy for zero_shot_rag: 67.95%\n",
    "Results for qwen2.5:7b\n",
    "Accuracy for zero_shot: 62.31%\n",
    "Accuracy for zero_shot_rag: 62.41%\n",
    "Results for phi4:14b\n",
    "Accuracy for zero_shot: 60.29%\n",
    "Accuracy for zero_shot_rag: 65.32%\n",
    "\n",
    "### BENCH (remove questions) ###\n",
    "Results for mistral\n",
    "Accuracy for zero_shot: 67.36%\n",
    "Accuracy for zero_shot_rag: 73.71%\n",
    "Results for llama3.1:8b\n",
    "Accuracy for zero_shot: 76.84%\n",
    "Accuracy for zero_shot_rag: 81.91%\n",
    "Results for llama2:7b\n",
    "Accuracy for zero_shot: 20.83%\n",
    "Accuracy for zero_shot_rag: 50.48%\n",
    "Results for gemma:7b\n",
    "Accuracy for zero_shot: 69.18%\n",
    "Accuracy for zero_shot_rag: 75.94%\n",
    "Results for gemma2:9b\n",
    "Accuracy for zero_shot: 80.99%\n",
    "Accuracy for zero_shot_rag: 83.56%\n",
    "Results for qwen2.5:7b\n",
    "Accuracy for zero_shot: 79.55%\n",
    "Accuracy for zero_shot_rag: 78.07%\n",
    "Results for phi4:14b\n",
    "Accuracy for zero_shot: 79.68%\n",
    "Accuracy for zero_shot_rag: 82.05%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_chart(bench, baseline, bar1, bar2, bar3, xl, yl, title):\n",
    "    # Creiamo un dizionario dalla lista baseline per una ricerca veloce\n",
    "    baseline_dict = {item[0]: item[1] for item in baseline}\n",
    "\n",
    "    # Uniamo le liste\n",
    "    merged_list = [item + [baseline_dict[item[0]]] for item in bench if item[0] in baseline_dict]\n",
    "\n",
    "    # Ordiniamo la lista in base al primo float (item[1])\n",
    "    merged_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(merged_list)\n",
    "\n",
    "    # Estrai le etichette e i valori\n",
    "    labels = [x[0] for x in merged_list]\n",
    "    values1 = [x[1] for x in merged_list]\n",
    "    values2 = [x[2] for x in merged_list]\n",
    "    values3 = [x[3] for x in merged_list]\n",
    "\n",
    "    # Imposta la posizione delle barre con pi√π spazio tra i gruppi\n",
    "    x = np.arange(len(labels)) * 1.3  # Moltiplica per aumentare la distanza tra i gruppi\n",
    "    width = 0.35  # Larghezza delle barre\n",
    "\n",
    "    # Aumenta le dimensioni del grafico\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    bars1 = ax.bar(x - width, values1, width, label=bar1)\n",
    "    bars2 = ax.bar(x, values2, width, label=bar2)\n",
    "    bars3 = ax.bar(x + width, values3, width, label=bar3)    \n",
    "\n",
    "    # Etichette e titolo\n",
    "    ax.set_xlabel(xl)\n",
    "    ax.set_ylabel(yl)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    # Mostra il grafico\n",
    "    plt.show()\n",
    "\n",
    "show_chart(bench, baseline, \"no RAG\", \"RAG\", \"Baseline\", \"Models\", \"Accuracy\", \"Benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import random\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def balance_correct_answer(csv_file, output_file):\n",
    "    # Carica il file CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Definisce le lettere per le opzioni (A, B, C, D, E)\n",
    "    option_letters = list(string.ascii_uppercase[:5])\n",
    "    \n",
    "    # Conta la distribuzione attuale delle risposte corrette\n",
    "    correct_counts = Counter(df['correct_option'])\n",
    "    \n",
    "    # Calcola il numero desiderato per ciascuna lettera\n",
    "    target_count = len(df) // 5\n",
    "    \n",
    "    # Inizializza un dizionario per tenere traccia delle assegnazioni\n",
    "    assigned_counts = {letter: 0 for letter in option_letters}\n",
    "    \n",
    "    # Funzione per ribilanciare la posizione della risposta corretta\n",
    "    def rebalance(row):\n",
    "        options = ast.literal_eval(row['options'].replace(\"['\", '[\"').replace(\"']\", '\"]').replace(\"', '\", '\", \"'))  # Converte la stringa in lista\n",
    "        correct_letter = row['correct_option']\n",
    "        correct_index = option_letters.index(correct_letter)\n",
    "        correct_answer = options[correct_index]\n",
    "        \n",
    "        # Trova le lettere meno usate\n",
    "        available_letters = [letter for letter in option_letters if assigned_counts[letter] < target_count]\n",
    "        \n",
    "        # Se tutte le lettere sono bilanciate, assegna a caso\n",
    "        new_correct_letter = random.choice(available_letters) if available_letters else random.choice(option_letters)\n",
    "        new_correct_index = option_letters.index(new_correct_letter)\n",
    "        \n",
    "        # Rimescola le risposte\n",
    "        random.shuffle(options)\n",
    "        \n",
    "        # Sposta la risposta corretta nella nuova posizione\n",
    "        options.remove(correct_answer)\n",
    "        options.insert(new_correct_index, correct_answer)\n",
    "        \n",
    "        # Aggiorna il conteggio\n",
    "        assigned_counts[new_correct_letter] += 1\n",
    "        \n",
    "        return pd.Series([str(options), new_correct_letter])\n",
    "    \n",
    "    # Applica la funzione a ogni riga\n",
    "    df[['options', 'correct_option']] = df.apply(rebalance, axis=1)\n",
    "    \n",
    "    # Salva il nuovo file CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full.csv\")\n",
    "print(\"Distribuzione prima:\")\n",
    "print(Counter(df[\"correct_option\"]))\n",
    "\n",
    "# # Esegui la funzione su un file di esempio\n",
    "# bal_df = balance_correct_answer(\"/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full.csv\", \"/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full_balanced.csv\")\n",
    "\n",
    "# # df = balance_correct_options(df)\n",
    "# print(\"Distribuzione dopo:\")\n",
    "# print(Counter(bal_df[\"correct_option\"]))\n",
    "\n",
    "# df.to_csv(\"/home/cc/PHD/HealthBranches/questions_pro/ultimate_questions_v3_full_balanced.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
