{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import argparse\n",
    "from typing import Dict, List\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "from classes.vector_store import VectorStore\n",
    "# from classes.llm_inference import LLMinference\n",
    "from classes.utils import check_results\n",
    "from prompt import *\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"LLM inference with different modalities.\")\n",
    "# parser.add_argument(\"-base\", action=\"store_true\", help=\"Run in baseline mode.\")\n",
    "# parser.add_argument(\"-quiz\", action=\"store_true\", help=\"Run in quiz mode.\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Set BASELINE based on the argument\n",
    "# BASELINE = args.base\n",
    "# QUIZ = args.quiz\n",
    "BASELINE = False\n",
    "QUIZ = True\n",
    "PATH = \"/home/cc/PHD/HealthBranches/\"\n",
    "EXT = \"QUIZ\" if QUIZ else \"OPEN\"\n",
    "\n",
    "print(\"##### BASELINE MODE #####\\n\" if BASELINE else \"##### BENCHMARK MODE #####\\n\")\n",
    "print(\"##### QUIZ EXP #####\\n\" if QUIZ else \"##### OPEN EXP #####\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "class LLMinference:\n",
    "    def __init__(self, llm_name: str, temperature: float = 0.01, num_predict: int = 128, device: int = 0):\n",
    "        self.llm_name = llm_name\n",
    "        self.temperature = temperature\n",
    "        self.num_predict = num_predict\n",
    "        # Load tokenizer and model in FP16 (if using GPU; for CPU set device=-1 and remove torch_dtype)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        # Create a text-generation pipeline; set device=0 for GPU (or -1 for CPU)\n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    # def _remove_reasoning(self, text):\n",
    "    #     \"\"\"\n",
    "    #     Remove any chain-of-thought reasoning enclosed in <think>...</think> tags.\n",
    "    #     \"\"\"\n",
    "    #     return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    def _remove_reasoning(self, text):\n",
    "        \"\"\"\n",
    "        Remove all text before the closing </think> tag.\n",
    "        If the tag is found, returns the text after the tag; otherwise, returns the original text.\n",
    "        \"\"\"\n",
    "        pos = text.find(\"</think>\")\n",
    "        if pos != -1:\n",
    "            return text[pos + len(\"</think>\"):].strip()\n",
    "        return text.strip()\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Invoke the model with the given prompt and return the generated text.\n",
    "        \"\"\"\n",
    "        d = \"\"\"You are a professional assistant. Answer the question directly and do not include any internal reasoning or chain-of-thought.\\n \"\"\"+prompt\n",
    "        outputs = self.generator(d, max_new_tokens=self.num_predict, temperature=self.temperature, do_sample=True)\n",
    "        # Extract generated text from the first output\n",
    "        # print(outputs)\n",
    "        for output in outputs:\n",
    "            answer = output['generated_text']\n",
    "            final_answer = self._remove_reasoning(answer)\n",
    "            # print(\"Final Answer:\")\n",
    "            # print(final_answer)\n",
    "\n",
    "        response_text = final_answer\n",
    "        return response_text\n",
    "\n",
    "    def single_inference(self, query: str, template: str, path: str, text: str, choices: List[str], cond: str, context):\n",
    "        # Join context documents (assumed to be a list of objects with page_content attribute)\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in context])\n",
    "        prompt_template = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        if choices:  # quiz mode\n",
    "            if path != \"\" and text != \"\":\n",
    "                prompt = prompt_template.format(\n",
    "                    context=context_text,\n",
    "                    question=query,\n",
    "                    path=path,\n",
    "                    text=text,\n",
    "                    condition=cond,\n",
    "                    o1=choices[0],\n",
    "                    o2=choices[1],\n",
    "                    o3=choices[2],\n",
    "                    o4=choices[3],\n",
    "                    o5=choices[4]\n",
    "                )\n",
    "            else:\n",
    "                prompt = prompt_template.format(\n",
    "                    context=context_text,\n",
    "                    question=query,\n",
    "                    condition=cond,\n",
    "                    o1=choices[0],\n",
    "                    o2=choices[1],\n",
    "                    o3=choices[2],\n",
    "                    o4=choices[3],\n",
    "                    o5=choices[4]\n",
    "                )\n",
    "        else:  # open question\n",
    "            if path != \"\" and text != \"\":\n",
    "                prompt = prompt_template.format(\n",
    "                    context=context_text,\n",
    "                    question=query,\n",
    "                    path=path,\n",
    "                    text=text,\n",
    "                    condition=cond\n",
    "                )\n",
    "            else:\n",
    "                prompt = prompt_template.format(\n",
    "                    context=context_text,\n",
    "                    question=query,\n",
    "                    condition=cond\n",
    "                )\n",
    "\n",
    "        response_text = self.invoke(prompt)\n",
    "        response_text = response_text.strip().replace(\"\\n\", \"\").replace(\"  \", \"\")\n",
    "        sources = [doc.metadata.get(\"source\", None) for doc in context]\n",
    "        \n",
    "        return response_text, sources\n",
    "\n",
    "    def qea_evaluation(self, query: str, template: str, path: str, txt: str, choices: List[str], cond: str, vector_store, k: int = 3) -> str:\n",
    "        results = vector_store.search(query=query, k=k)\n",
    "        response, sources = self.single_inference(query, template, path, txt, choices, cond, results)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty vector store in the indicated path. If the path already exists, load the vector store\n",
    "vector_store = VectorStore(f'{PATH}indexes/kgbase-new/')\n",
    "\n",
    "# Add documents in vector store (comment this line after the first add)\n",
    "# vector_store.add_documents('/home/cc/PHD/ragkg/data/kgbase')\n",
    "\n",
    "# folder_path = f\"{PATH}questions_pro/ultimate_questions_v3_full_balanced.csv\"\n",
    "folder_path = f\"{PATH}questions_pro/dataset_updated.csv\"\n",
    "questions = pd.read_csv(folder_path)\n",
    "\n",
    "models = [\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"]\n",
    "\n",
    "models = check_results(PATH+\"results/\", f\"results_{EXT}_baseline_*.csv\" if BASELINE else f\"results_{EXT}_bench_*.csv\", models)\n",
    "\n",
    "templates = [PROMPT_QUIZ, PROMPT_QUIZ_RAG] if QUIZ else [PROMPT_OPEN, PROMPT_OPEN_RAG]\n",
    "\n",
    "if BASELINE:\n",
    "    templates = [PROMPT_QUIZ_BASELINE] if QUIZ else [PROMPT_OPEN_BASELINE]\n",
    "\n",
    "cnt_rag = 0\n",
    "cnt = 0\n",
    "\n",
    "rows = []\n",
    "questions = pd.read_csv(folder_path)\n",
    "\n",
    "for model_name in models:\n",
    "    llm = LLMinference(llm_name=model_name)\n",
    "\n",
    "    cnt = 0\n",
    "    rows = []\n",
    "    print(f\"Running model {model_name}...\")\n",
    "    with alive_bar(len(questions)) as bar:\n",
    "        for index, row in questions[:5].iterrows():\n",
    "            res = []\n",
    "            opts = []\n",
    "\n",
    "            try:\n",
    "                opts = ast.literal_eval(row['options'].replace(\"['\", '[\"').replace(\"']\", '\"]').replace(\"', '\", '\", \"'))\n",
    "                if not isinstance(opts, list) or len(opts) != 5:\n",
    "                    print(f\"Skipping row {index} due to invalid options\")\n",
    "                    continue  # Skip this iteration if the condition is not met\n",
    "\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Skipping row {index} due to value/syntax error\")\n",
    "                continue  # Skip if there's an issue with evaluation\n",
    "\n",
    "            txt_name = row['condition'].upper()+\".txt\"\n",
    "            txt_folder_name = f\"{PATH}data/kgbase-new/\"\n",
    "\n",
    "            try:\n",
    "                with open(os.path.join(txt_folder_name, txt_name), 'r') as file:\n",
    "                    text = file.readlines()\n",
    "            except Exception:\n",
    "                print(os.path.join(txt_folder_name, txt_name))\n",
    "                print(f\"{txt_name} text is EMPTY!\")\n",
    "                continue    \n",
    "            \n",
    "            for template in templates:\n",
    "                if BASELINE:\n",
    "                    try:\n",
    "                        res.append(llm.qea_evaluation(row['question'], template, row['path'], text, opts, row['condition'].lower(), vector_store)) # Baseline\n",
    "                    except Exception:\n",
    "                        print(row)\n",
    "                else:\n",
    "                    try:\n",
    "                        res.append(llm.qea_evaluation(row['question'], template, \"\", \"\", opts, row['condition'].lower(), vector_store))\n",
    "                    except Exception as e:\n",
    "                        print(row)\n",
    "\n",
    "            if QUIZ:\n",
    "                res.append(row[\"correct_option\"])\n",
    "            else:\n",
    "                res.append(opts[ord(row[\"correct_option\"].strip().upper()) - ord('A')])\n",
    "\n",
    "            res.append(row['question'])\n",
    "            res.append(row['path'])\n",
    "            res.insert(0, row['condition'].lower())\n",
    "\n",
    "            rows.append(res)\n",
    "            bar()\n",
    "\n",
    "        if BASELINE:\n",
    "            df = pd.DataFrame(rows, columns=[\"name\", \"zero_shot\", \"real\", \"question\", \"path\"]) # Baseline\n",
    "            df.to_csv(f\"{PATH}/results/results_{EXT}_baseline_{model_name}.csv\", index=False) # Baseline\n",
    "        else:\n",
    "            df = pd.DataFrame(rows, columns=[\"name\", \"zero_shot\", \"zero_shot_rag\", \"real\", \"question\", \"path\"])\n",
    "            df.to_csv(f\"{PATH}/results/results_{EXT}_bench_DeepSeek-R1-Distill-Qwen-7B.csv\", index=False)\n",
    "\n",
    "    print(f\"Model {model_name} done!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_reasoning(text):\n",
    "#     \"\"\"\n",
    "#     Remove any chain-of-thought reasoning enclosed in <think>...</think> tags.\n",
    "#     \"\"\"\n",
    "#     return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "# # Specify the model ID from Hugging Face Hub.\n",
    "# model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "# # Load the tokenizer.\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "\n",
    "# # Load the model in FP16 (half-precision) mode.\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "\n",
    "# # Create a text-generation pipeline; set device=0 if using GPU.\n",
    "# generator = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=0,\n",
    "#     trust_remote_code=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "\n",
    "# # Define the prompt with an instruction to not reveal internal reasoning.\n",
    "# prompt = (\n",
    "#     \"You are a professional assistant. Answer the question directly and do not include any internal reasoning or chain-of-thought. \"\n",
    "#     \"Who are you?\"\n",
    "# )\n",
    "\n",
    "# # Generate the model response.\n",
    "# outputs = generator(prompt, max_new_tokens=100, do_sample=False)\n",
    "\n",
    "# # Process and print the final answer, filtering out any chain-of-thought text.\n",
    "# for output in outputs:\n",
    "#     answer = output['generated_text']\n",
    "#     final_answer = remove_reasoning(answer)\n",
    "#     print(\"Final Answer:\")\n",
    "#     print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
